# from llama_cpp import Llama
# from tqdm import tqdm
# import os
# import json
#
# # LLM ì§ˆì˜ ì²˜ë¦¬ í•¨ìˆ˜
# def process_ask_to_llm(raw_text: str):
#     print("ğŸ“˜ 1ë‹¨ê³„: ì˜¤íƒˆì ë° ëª…ì¹­ ì •ê·œí™” ì¤‘...")
#     corrected = correct_typos(raw_text)
#     print(corrected)
#
#     # print("ğŸ“— 2ë‹¨ê³„: JSON ì¶”ì¶œ ì¤‘...")
#     # json_text = extract_whisky_json(corrected)
#     #
#     # print("\nâœ… ìµœì¢… ê²€ì¦ëœ JSON ê²°ê³¼:")
#     # for item in json_text:
#     #     print(json.dumps(item, ensure_ascii=False, indent=2))
#
#     return corrected
#
# # ëª¨ë¸ ê²½ë¡œ ì„¤ì •
# model_path = "/mnt/d/models/DevQuasar/allenai.Llama-3.1-Tulu-3-8B-GGUF/allenai.Llama-3.1-Tulu-3-8B.Q4_K_S.gguf"
#
# if not os.path.exists(model_path):
#     raise FileNotFoundError(f"ëª¨ë¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
#
# llm = Llama(
#     model_path=model_path,
#     n_ctx=7700,
#     n_threads=6,
#     use_mlock=False,
#     use_mmap=True,
#     keep_in_memory=True
# )
#
# # í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í• 
# def split_text_into_chunks(text: str, max_chars: int = 1000):
#     return [text[i:i + max_chars] for i in range(0, len(text), max_chars)]
#
# # 1ë‹¨ê³„: ì˜¤íƒˆì ë° ëª…ì¹­ ì •ê·œí™”
# def correct_typos(text: str) -> str:
#     chunks = split_text_into_chunks(text, max_chars=1800)
#     corrected_chunks = []
#     for chunk in tqdm(chunks, desc="ğŸ” ì •ê·œí™” ì²˜ë¦¬ ì¤‘"):
#         prompt = (
#             "ë‹¤ìŒ í…ìŠ¤íŠ¸ëŠ” ìœ„ìŠ¤í‚¤ ê´€ë ¨ ì •ë³´ì•¼. ì˜¤íƒˆìë‚˜ ì˜ëª»ëœ í‘œê¸° (ìœ„ìŠ¤í‚¤ ì´ë¦„ì´ë‚˜ ë¸Œëœë“œ)ë¥¼ "
#             "ì •í™•í•˜ê²Œ ìˆ˜ì •í•´ì¤˜. ê²°ê³¼ëŠ” ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ ì¶œë ¥í•´ì¤˜.\n\n"
#             f"```{chunk}```"
#         )
#         result = llm(
#             prompt,
#             temperature=0.3,
#             top_p=0.95,
#             max_tokens=2048,
#         )
#         corrected_chunks.append(result["choices"][0]["text"].strip())
#
#     return "\n".join(corrected_chunks)
#
# # 2ë‹¨ê³„: JSON ì¶”ì¶œ
# def extract_whisky_json(text: str) -> list:
#     chunks = split_text_into_chunks(text, max_chars=1800)
#     json_results = []
#     for chunk in tqdm(chunks, desc="ğŸ“¦ JSON ì¶”ì¶œ ì¤‘"):
#         prompt = (
#             "ë‹¤ìŒì€ ì •ì œëœ ìœ„ìŠ¤í‚¤ ì •ë³´ì•¼. ì—¬ê¸°ì—ì„œ ìœ„ìŠ¤í‚¤ ì´ë¦„_ì—°ë„í¬í•¨(name_age), ìš©ëŸ‰(volume_ml), "
#             "ê°€ê²©(price_krw), ë‚ ì§œ(date)ë¥¼ í¬í•¨í•œ JSON ê°ì²´ë“¤ì„ ì¶”ì¶œí•´ì¤˜. "
#             "ë³µìˆ˜ í•­ëª©ì´ë¼ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶œë ¥í•´ì¤˜.\n\n"
#             "ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´ì¤˜. ì˜ˆì‹œëŠ” ë‹¤ìŒê³¼ ê°™ì•„:\n\n"
#             "[\n"
#             "  {\n"
#             "    \"name_age\": \"Glenfiddich 12\",\n"
#             "    \"volume_ml\": 700,\n"
#             "    \"price_krw\": 65000,\n"
#             "    \"date\": \"2024-05-25\"\n"
#             "  }\n"
#             "]\n\n"
#             f"```{chunk}```"
#         )
#         result = llm(
#             prompt,
#             temperature=0.3,
#             top_p=0.95,
#             max_tokens=2048,
#         )
#         json_results.append(result["choices"][0]["text"].strip())
#
#     # JSON íŒŒì‹± (ê²€ì¦ ë° ë³€í™˜)
#     parsed_results = []
#     for item in json_results:
#         try:
#             parsed = json.loads(item)
#             if isinstance(parsed, list):
#                 parsed_results.extend(parsed)
#             else:
#                 parsed_results.append(parsed)
#         except json.JSONDecodeError:
#             print("âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨:", item)
#
#     return parsed_results
